---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# dbloadss

This package and accompanying scripts is to test load times to MS SQL Server 17 (SS) from various methods:

1. Push from R with `RODBC`
2. Push from R with `odbc`
3. Pull from SS with a stored procedure

This package is really for number 3. We can do the rest from a script, although it's a useful place to keep some functions.

# Deploying R Models in SQL Server

As an R user who is building models and analysing data one of the key challenges is how do you make those results available to those who need it? For reporting there are many options from writing [Excel files](https://www.mango-solutions.com/blog/r-the-excel-connection) to [rmarkdown documents](https://rmarkdown.rstudio.com/) and [shiny apps](https://shiny.rstudio.com/). A common situation is where results will be integrated into a business intelligence (BI) tool alongside a number of other critcial business metrics. In this situation you will be working with SQL developers to integrate your work. The question is, what is the best way to deliver R code to the BI team?

In this blog post we will be looking at the specific case of deploying a predictive model, written in R, to a Microsoft SQL Server database for consumption by a BI tool. We will assume that the model runs in batch unattended, as a scheduled job.


set up the simulation problem

## Use Packages

At Mango we believe that the basic unit of work is an R package. A well written R package will be self-documenting, have a familiar structure, and unit tests. All behind-the-scenes code can be written into unexported functions, and user facing code live in a small number (often one) of exported functions. This single entry point should be designed for someone who is not an R user to run the code, and if anything goes wrong, be as informative as possible.

## Output Everything



## Push or Pull?

Once the model has been packaged and the interface decided, it remains to decide how to actually run the code. With SQL Server there are two options:

1. Run the model from R and *push* the results to SQL Server using an ODBC connection.
2. Call the model from SQL Server using a stored procedure to run an R script using R Services.






# Test Database

The code for this post runs on my Windows 10 laptop, where I have a local SQL Server 17 instance running, with a database called `ml`.

# Load Times

We're going to push a big data frame into SQL Server using three methods. The data set is a simple randomly generated data frame.

```{r rand_data}
library(dbloadss)

random_data_set(n_rows = 5, n_cols = 5)
```

R is pretty quick at this sort of thing so we don't really need to worry about how long it takes to make a big data frame.

```{r time_create}
n_rows <- 3000000
system.time({
  random_data <- random_data_set(n_rows = n_rows, n_cols = 5)
})
```

but what we're interested in is how fast to push this to SQL Server?.

## RODBC

`RODBC` was, for a long time, the standard way to connect to SQL Server from R. It's a great package that makes it easy to send queries, collect results, and handles type conversions pretty well. However, it is a bit slow for pushing data in. The fastest I could manage was using the `sqlSave` function with the safeties off. Very interested to hear if there's a better method. For 3m rows it's a no go. So scaling back to 30k rows we get:

```{r rodbc}
library(RODBC)

db <- odbcDriverConnect('driver={SQL Server};server=localhost\\SQL17ML;database=ml;trusted_connection=true')

n_rodbc <- 30000

odbcQuery(db, "drop table randData;")
time30k <- system.time({
  RODBC::sqlSave(
    db,
    dat = random_data[1:n_rodbc,],
    tablename = "randData",
    rownames = FALSE,
    fast = TRUE,
    safer = FALSE
  )
})
odbcClose(db)

time30k
```

Over 2 minutes! It's been roughly linear for me so that total write time for 3m rows is a few hours.

## ODBC

`odbc` is a relatively new package from RStudio which provides a DBI compliant ODBC interface. It is considerably faster for writes. Here we'll push the full 3m rows.

```{r odbc}
library(DBI)

dbi <- dbConnect(odbc::odbc(),
                 driver = "SQL Server",
                 server="localhost\\SQL17ML",
                 database = "ml")

time3modbc <- system.time({
  dbWriteTable(dbi,
               name = "randData",
               value = random_data,
               overwrite = TRUE)
})
dbDisconnect(dbi)

time3modbc
```

## SQL Server External Script

An alternative approach is to use the new features in SQL Server 17 (and 16) for calling out to R scripts from SQL. This is done via the `sp_execute_external_script` command, which we will wrap in a stored procedure. This is what that looks like for me:

```sql
use [ml];

DROP PROC IF EXISTS generate_random_data;
GO
CREATE PROC generate_random_data(@nrow int)
AS
BEGIN
 EXEC sp_execute_external_script
       @language = N'R'  
     , @script = N'  
          library(dbloadss)
          random_data <- random_data_set(n_rows = nrow_r, n_cols = 5)
' 
   , @output_data_1_name = N'random_data'
	 , @params = N'@nrow_r int'
	 , @nrow_r = @nrow
    WITH RESULT SETS ((
	      "COL_1" float not null,   
        "COL_2" float not null,  
        "COL_3" float not null,   
        "COL_4" float not null,
		    "COL_5" float not null)); 
END;
GO
```

We then call the stored procedure with another query (skipping out a step that clears it inbetween tests).

```sql
INSERT INTO randData
EXEC [dbo].[generate_random_data] @nrow = 3000000

```

![SQL Server Timer](SStime.jpg)

and this runs in 34 seconds. My best guess for the performance increase is that the data is serialised more efficiently. More to investigate.


# License 

MIT Â© Mango Solutions
