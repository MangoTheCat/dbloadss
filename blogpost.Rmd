---
title: "Deploying R Models in SQL Server"
author: "Doug Ashton"
date: "23 May 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

consql <- DBI::dbConnect(odbc::odbc(),
                 driver = "SQL Server",
                 server="localhost\\SQL17ML",
                 database = "ml")
```


As an R user who is building models and analysing data one of the key challenges is how do you make those results available to those who need it? After all, [data science is about making better decisions](https://www.mango-solutions.com/blog/a-definition-of-data-science), and your results need to get into the hands of the people who make those decisions.

For reporting there are many options from writing [Excel files](https://www.mango-solutions.com/blog/r-the-excel-connection) to [rmarkdown documents](https://rmarkdown.rstudio.com/) and [shiny apps](https://shiny.rstudio.com/). Many businesses will require results to go into a business intelligence (BI) tool alongside a number of other critcial business metrics. Moreover the results need to be refreshed daily. In this situation you will be working with SQL developers to integrate your work. The question is, what is the best way to deliver R code to the BI team?

In this blog post we will be looking at the specific case of deploying a predictive model, written in R, to a Microsoft SQL Server database for consumption by a BI tool. We'll look at some of the different options to integrate R, from in-database R services, to pushing from with [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity) or flat files from [SSIS](https://docs.microsoft.com/en-gb/sql/integration-services/sql-server-integration-services).

# The Problem

## Flight delay planning

To demonstrate we'll use the familiar `flights` dataset from the [nycflights13](https://CRAN.R-project.org/package=nycflights13) package to imagine that we are airport planners and we want to test various scenarios related to flight delays. Our data contains the departure delay of all flights leaving the New York airports: JFK, LGA, and EWR in 2013. I'm running this code on my Windows 10 laptop, where I have a local SQL Server 17 instance running, with a database called `ml`. If you want to reproduce the code you'll need to have your own SQL Server setup (you can install it locally) and push the flights table there. Here's a selection of columns:

```{sql, peakflights, connection=consql}
SELECT TOP(5) flight, origin, dest, sched_dep_time, carrier, time_hour, dep_delay
FROM flights
```

We'll fit a statistical model for the departure delay, and run simulations for the delay of future flights. We want to capture the natural variation from day to day so a useful approach here is a mixed-effects model where each day is a random effect.

```{r, themodel, eval=FALSE}
model <- lme4::lmer(
    dep_delay ~ 1 +
      (1 | date:origin) +
      carrier +
      origin +
      sched_dep_time +
      distance +
      week_day,
    data = data_train
  )
```

This is a simple model for demonstration purposes. For example, it doesn't capture big delays (extreme values) well, but it will serve our purpose. The full model code and data prep is available at [mangothecat/dbloadss](https://github.com/mangothecat/dbloadss) so we won't go through every line here.

To simulate delays on future flights we can call the `simulate` function. Here we'll run 10 different scenarios:

```{r, eval = FALSE}
sim_delays <- simulate(model, nsim = 10, newdata = data_test)
```

The reason we're using `simulate` rather than `predict` is that we don't just want the most likely value for each delay, we want to sample from likely scenarios.

# Implementation

The data scientist has done their exploratory work, made some nice notebooks, and are really happy with their p-values. How do we now deploy their model?

## Use Packages

At Mango we believe that the basic unit of work is a package. A well written package will be self-documenting, have a familiar structure, and unit tests. All behind-the-scenes code can be written into unexported functions, and user facing code lives in a small number (often one) of exported functions. This single entry point should be designed for someone who is not an R user to run the code, and if anything goes wrong, be as informative as possible.

The code for this blog post lives in the [dbloadss](https://github.com/mangothecat/dbloadss) package available on GitHub. For the flights model a single function is exported `simulate_departure_delays`, which is documented to explain exactly what it expects as input, and what it will output. The entire model runs with the single line:

```r
output_data <- simulate_departure_delays(input_data, nsim = 20)
```

where the `input_data` is prepared from the database and `output_data` will be pushed/pulled back to the database.

## Output Everything

A data scientist will ask: "how can I predict `dep_delay` as accurately as possible?". An airport manager will want to know "how often will the last flight of the day leave after midnight?", or another question that you haven't thought of. 

Of course we can use R to answer each of these questions one at a time. However, this is frustrating for everyone because the data scientist wants to be modelling, and the business user has to wait for each new answer.

Fortunately, this is exactly the kind of thing that SQL and BI tools are built to do. So instead of processing the results in R we will output every simulation run into SQL Server and do the post processing in the database or BI tool.

# Connecting to the Database

## Push, Pull, or Pickup?

Once the model has been packaged and the interface decided, it remains to decide how to actually run the code. With SQL Server there are three options:

1. Run the model from R and *push* the results to SQL Server using an ODBC connection.
2. Call the model from SQL Server using a stored procedure to run an R script using R Services and *pull* the results back.
3. Invoke an Rscript from SSIS and *pickup* flat files (csv).

Which you choose will depend on a number of factors. We'll take some time to look at each one.

## The Push (SQL from R)

The best way to talk to a database from R is to use the [DBI](http://r-dbi.github.io/DBI/) database interface package. The [DBI project](https://r-dbi.org.) has been around for a while but received a boost with R Consortium funding. It provides a common interface to many databases integrating specific backend packages to each separate database type. For SQL Server we're going to use the [odbc](https://CRAN.R-project.org/package=odbc) backend. It has [great documentation](https://db.rstudio.com/odbc/) and since Microsoft released [ODBC drivers for Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server) it's a  cinch to setup from most operating systems.

Let's get the flights data from SQL Server:

```{r, odbcload, message=FALSE, warning=FALSE}
library(DBI)
con <- dbConnect(odbc::odbc(),
                 driver = "SQL Server",
                 server = "localhost\\SQL17ML",
                 database = "ml")

flights <- dbReadTable(con, Id(schema="dbo", name="flights"))
```

I've included the the explicit `schema` argument because it's a recent addition to DBI and it can be a sticking point for complicated database structures.

Now we run the model as above

```{r, runmodelodbc, message=FALSE, warning=FALSE}
library(dbloadss)
output_data <- simulate_departure_delays(flights, nsim = 20, split_date = "2013-07-01")
dim(output_data)
```

So for 20 simulations we have about 3.5 million rows of output! It's just a flight ID (for joining back to the source), a simulation ID, and a delay.

```{r, modeloutput}
head(output_data)
```

We'll do all further processing in the database so let's push it back.

```{r, odbcpush, warn = FALSE, message = FALSE, cache = TRUE}
# Workaround for known issue https://github.com/r-dbi/odbc/issues/175
dbRemoveTable(con, name = Id(schema = "dbo", name = "flightdelays"))

odbctime <- system.time({
  dbWriteTable(con,
               name = Id(schema = "dbo", name = "flightdelays"),
               value = output_data,
               overwrite = TRUE)
})
odbctime
```

That took under 2 minutes. This post started life as a benchmark of write times from odbc vs [RODBC](https://CRAN.R-project.org/package=RODBC), an alternative way to talk to SQL Server. The results are on the [dbloadss README](https://github.com/mangothecat/dbloadss) and suggest this would take several hours! RODBC is usually fine for reads but we recommend switching to odbc where possible.

It is relatively straight forward to push from R and this could run as a scheduled job from a server running R.

## The Pull (R from SQL)

An alternative approach is to use the new features in SQL Server 17 (and 16) for calling out to R scripts from SQL. This is done via the `sp_execute_external_script` command, which we will wrap in a stored procedure. This method is great for SQL developers because they don't need to go outside their normal tool and they can have greater control about exactly what is returned and where it goes.

A word of warning before we continue. There's a line in the [Microsoft docs](https://docs.microsoft.com/en-us/sql/advanced-analytics/install/sql-r-services-windows-install?view=sql-server-2017):

> Do not install R Services on a failover cluster. The security mechanism used for isolating R processes is not compatible with a Windows Server failover cluster environment.

that somewhat limits the ultimate use of this technique in production settings as a failover cluster is a very common configuration. Assuming this might get fixed, or perhaps it's not an issue for you, let's see how it works.

The following stored procedure is what we'll add for our flight delays model:

```sql
use [ml];

DROP PROC IF EXISTS r_simulate_departure_delays;
GO
CREATE PROC r_simulate_departure_delays(
    @nsim int = 20,
    @split_date date = "2013-07-01")
AS
BEGIN
 EXEC sp_execute_external_script
     @language = N'R'  
   , @script = N'
    library(dbloadss)
    output_data <- simulate_departure_delays(input_data, nsim = nsim_r,
                                             split_date = split_date_r)
' 
   , @input_data_1 = N' SELECT * FROM [dbo].[flights];'
   , @input_data_1_name = N'input_data'
   , @output_data_1_name = N'output_data'
   , @params = N'@nsim_r int, @split_date_r date'
   , @nsim_r = @nsim
   , @split_date_r = @split_date
    WITH RESULT SETS ((
	    "id" int not null,   
        "sim_id" int not null,  
        "dep_delay" float not null)); 
END;
GO
```

The query that goes into `@input_data_1` becomes a data frame in your R session. The main things to note are that you can pass in as many parameters as you like, but only *one* data frame. Your R script assigns the results to a nominated output data frame and this is picked up and returned to SQL server. 

I believe it's very important that the R script that is inside the stored procedure does not get too complicated. Much better to use your single entry point and put complex code in a package where it can be unit tested and documented.

We then call the stored procedure with another query:

```sql
INSERT INTO [dbo].[flightdelays]
EXEC [dbo].[r_simulate_departure_delays] @nsim = 20
```

The perforance of this method seems to be good. For write speeds in our tests it was faster even than pushing with odbc, although it's harder to benchmark in the flights example because it includes running the simulation.

Overall, were it not for the issue with failover clusters I would be recommending this as the best way to integrate R with SQL Server. As it stands you'll have to evaluate on your setup.

## The Pickup (R from SSIS)

The final method is to use [SSIS](https://docs.microsoft.com/en-gb/sql/integration-services/sql-server-integration-services) to treat running the R model as an [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) process. To keep things simple we use SSIS to output the input data as a flat file (csv), kick-off an R process to run the job, and *pickup* the results from another csv. This means that we'll be making our R code run as a commandline tool and using a csv "air gap".

Running R from the command line is relatively straight forward. To handle parameters we've found the best way is to use [argparser](https://CRAN.R-project.org/package=argparser), also honourable mention to [optparse](https://CRAN.R-project.org/package=optparse). Checkout [Mark's blog post series on building R command line applications](http://blog.sellorm.com/2017/12/18/learn-to-write-command-line-utilities-in-r/). After you've parsed the arguments everything is essentially the same as pushing straight to the database, except that you write to csv at the end.

The SSIS solution has some great advantages in that it is controlled by the SQL developers, it has the greatest separation of technologies, and it's easy to test the R process in isolation. Downsides are it's unlikely that going via csv will be the fastest, and you need to be a little more careful about data types.

# The Results

Over to the BI team. Show a plot.
