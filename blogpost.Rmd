---
title: "Deploying R Models in SQL Server"
author: "Doug Ashton"
date: "23 May 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

consql <- DBI::dbConnect(odbc::odbc(),
                 driver = "SQL Server",
                 server="localhost\\SQL17ML",
                 database = "ml")
```


## Introduction

As an R user who is building models and analysing data one of the key challenges is how do you make those results available to those who need it? After all, data science is about making better decisions, and your results need to get into the hands of the people who make those decisions.

For reporting there are many options from writing [Excel files](https://www.mango-solutions.com/blog/r-the-excel-connection) to [rmarkdown documents](https://rmarkdown.rstudio.com/) and [shiny apps](https://shiny.rstudio.com/). Many businesses will require results to go into a business intelligence (BI) tool alongside a number of other critcial business metrics. Moreover the results need to be refreshed daily. In this situation you will be working with SQL developers to integrate your work. The question is, what is the best way to deliver R code to the BI team?

In this blog post we will be looking at the specific case of deploying a predictive model, written in R, to a Microsoft SQL Server database for consumption by a BI tool. We'll look at some of the different options to integrate R, from in-database R services, to pushing from with [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity) or flat files from [SSIS](https://docs.microsoft.com/en-gb/sql/integration-services/sql-server-integration-services).

### Flight delay planning

To demonstrate we'll use the `flights` dataset from the [nycflights13](https://CRAN.R-project.org/package=nycflights13) package to imagine that we are airport planners and we want to test various scenarios related to flight delays. Our data contains the departure delay of all flights leaving the New York airports: JFK, LGA, and EWR in 2013. We've already loaded the dataset into SQL Server. Below is a selection of columns.

```{sql, peakflights, connection=consql}
SELECT TOP(5) flight, origin, dest, sched_dep_time, carrier, time_hour, dep_delay
FROM flights
```

We'll fit a statistical model for the departure delay, and run simulations for the delay of future flights. We want to capture the natural variation from day to day so a useful approach here is a mixed-effects model where each day is a random effect.

```{r, themodel, eval=FALSE}
model <- lme4::lmer(
    dep_delay ~ 1 +
      (1 | date:origin) +
      carrier +
      origin +
      sched_dep_time +
      distance +
      week_day,
    data = data_train
  )
```

This is a simple model for demonstration purposes. For example, it doesn't capture big delays (extreme values) well, but it will serve our purpose. The full model code and data prep is available at [mangothecat/dbloadss](https://github.com/mangothecat/dbloadss) so we won't go through every line here.

## Implementation

The data scientist has done their exploratory work, made some nice notebooks, and are really happy with their p-values. How do we now deploy their model?

### Use Packages

At Mango we believe that the basic unit of work is a package. A well written package will be self-documenting, have a familiar structure, and unit tests. All behind-the-scenes code can be written into unexported functions, and user facing code lives in a small number (often one) of exported functions. This single entry point should be designed for someone who is not an R user to run the code, and if anything goes wrong, be as informative as possible.

The code for this blog post lives in the [dbloadss](https://github.com/mangothecat/dbloadss) package available on GitHub. For the flights model a single function is exported `simulate_departure_delays`, which is documented to explain exactly what it expects as input, and what it will output. The entire model runs with the single line:

```r
output_data <- simulate_departure_delays(input_data, nsim = 20)
```

where the `input_data` is prepared from the database and `output_data` will be pushed/pulled back to the database.

### Output Everything

A data scientist will ask: "how can I predict `dep_delay` as accurately as possible?". An airport manager will want to know "how often will the last flight of the day leave after midnight?", or another question that you haven't thought of. 

Of course we can use R to answer each of these questions one at a time. However, this is frustrating for everyone because the data scientist wants to be modelling, and the business user has to wait for each new answer.

Fortunately, this is exactly the kind of thing that SQL and BI tools are built to do. So instead of processing the results in R we will output every simulation run into SQL Server and do the post processing in the database or BI tool.

### Push, Pull, or Pickup?

Once the model has been packaged and the interface decided, it remains to decide how to actually run the code. With SQL Server there are three options:

1. Run the model from R and *push* the results to SQL Server using an ODBC connection.
2. Call the model from SQL Server using a stored procedure to run an R script using R Services and *pull* the results back.
3. Invoke an Rscript from SSIS and *pickup* flat files (csv).

Which you choose will depend on a number of factors. We'll take some time to look at each one.

## The Push (SQL from R)

The best way to talk to a database from R is to use the [DBI](http://r-dbi.github.io/DBI/) database interface package. The [DBI project](https://r-dbi.org.) has been around for a while but received a boost with R Consortium funding. It provides a common interface to many databases integrating specific backend packages to each separate database type. For SQL Server we're going to use the [odbc](https://CRAN.R-project.org/package=odbc) backend. It has [great documentation](https://db.rstudio.com/odbc/) and since Microsoft released [ODBC drivers for Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server) it's a  cinch to setup from most operating systems.

Let's get the flights data from SQL Server. I'm running this code on my Windows 10 laptop, where I have a local SQL Server 17 instance running, with a database called `ml`.

```{r, odbcload}
library(DBI)
con <- dbConnect(odbc::odbc(),
                 driver = "SQL Server",
                 server = "localhost\\SQL17ML",
                 database = "ml")

flights <- dbReadTable(con, Id(schema="dbo", name="flights"))
```

I've included the the explicit `schema` argument because it's a recent addition to DBI and it can be a sticking point for complicated database structures.

Now we run the model as above

```{r, runmodelodbc}
library(dbloadss)
output_data <- simulate_departure_delays(flights, nsim = 20, split_date = "2013-07-01")
dim(output_data)
```

So for 20 simulations we have about 3.5 million rows of output! It's just a flight ID (for joining back to the source), a simulation ID, and a delay.

```{r, modeloutput}
head(output_data)
```

We'll do all further processing in the database so let's push it back.

```{r, odbcpush, warn = FALSE, message = FALSE}
# Workaround for known issue https://github.com/r-dbi/odbc/issues/175
dbRemoveTable(con, name = Id(schema = "dbo", name = "flightsdelays"))

odbctime <- system.time({
  dbWriteTable(con,
               name = Id(schema = "dbo", name = "flightsdelays"),
               value = output_data,
               overwrite = TRUE)
})
odbctime
```

That took under 2 minutes. This post started life as a benchmark of write times from odbc vs [RODBC](https://CRAN.R-project.org/package=RODBC), an alternative way to talk to SQL Server. The results are on the [dbloadss README](https://github.com/mangothecat/dbloadss) and suggest this would take several hours! RODBC is usually fine for reads but we recommend switching to odbc where possible.

It is relatively straight forward to push from R and this could run as a scheduled job from a server running R.

## The Pull (R from SQL)


An alternative approach is to use the new features in SQL Server 17 (and 16) for calling out to R scripts from SQL. This is done via the `sp_execute_external_script` command, which we will wrap in a stored procedure. This is what that looks like for me:

```sql
use [ml];

DROP PROC IF EXISTS r_simulate_departure_delays;
GO
CREATE PROC r_simulate_departure_delays(
    @nsim int = 20,
    @split_date date = "2013-07-01")
AS
BEGIN
 EXEC sp_execute_external_script
     @language = N'R'  
   , @script = N'
    library(dbloadss)
    output_data <- simulate_departure_delays(input_data, nsim = nsim_r,
                                             split_date = split_date_r)
' 
   , @input_data_1 = N' SELECT * FROM [dbo].[flights];'
   , @input_data_1_name = N'input_data'
   , @output_data_1_name = N'output_data'
   , @params = N'@nsim_r int, @split_date_r date'
   , @nsim_r = @nsim
   , @split_date_r = @split_date
    WITH RESULT SETS ((
	    "id" int not null,   
        "sim_id" int not null,  
        "dep_delay" float not null)); 
END;
GO
```

We then call the stored procedure with another query (skipping out a step that clears it inbetween tests).

```sql
INSERT INTO [dbo].[flightdelays]
EXEC [dbo].[r_simulate_departure_delays] @nsim = 20
```

and this runs in 34 seconds. My best guess for the performance increase is that the data is serialised more efficiently. More to investigate.

## The Pickup (R from SSIS)


